/**
 * Queue Consumer Worker for Virtual Index Operations
 *
 * This worker processes jobs from the vitess-index-jobs queue:
 * - IndexBuildJob: Build a new virtual index from existing data
 * - IndexMaintenanceJob: Maintain indexes after UPDATE/DELETE operations
 */

import { withLogTags } from 'workers-tagged-logger';
import { logger } from './logger';
import type { IndexJob, IndexBuildJob, IndexMaintenanceJob, ReshardTableJob, ReshardingChangeLog, MessageBatch } from './engine/queue/types';
import type { Storage, QueryResult } from './engine/storage';
import type { Topology } from './engine/topology/index';
import { hashToShard } from './engine/utils/sharding';

/**
 * Queue message handler
 * Receives batches of up to 10 messages and processes them
 */
export async function queueHandler(batch: MessageBatch<IndexJob>, env: Env, batchCorrelationId?: string): Promise<void> {
	return withLogTags({ source: 'QueueConsumer' }, async () => {
		const cid = batchCorrelationId || crypto.randomUUID();
		logger.setTags({
			correlationId: cid,
			requestId: cid,
			component: 'QueueConsumer',
			operation: 'queueHandler',
		});

		logger.info('Processing queue batch', {
			batchSize: batch.messages.length,
			queue: batch.queue,
		});

		// Process messages in parallel where possible
		const results = await Promise.allSettled(
			batch.messages.map(async (message) => {
				try {
					const jobCorrelationId = message.body.correlation_id || cid;
					logger.setTags({
						correlationId: jobCorrelationId,
						requestId: jobCorrelationId,
						jobId: message.id,
						jobType: message.body.type,
					});

					logger.info('Processing queue message', {
						jobId: message.id,
						jobType: message.body.type,
						attempts: message.attempts,
					});

					await processIndexJob(message.body, env, jobCorrelationId);

					logger.info('Successfully processed job', {
						jobId: message.id,
						jobType: message.body.type,
						status: 'success',
					});
				} catch (error) {
					logger.error('Failed to process job', {
						jobId: message.id,
						jobType: message.body.type,
						error: error instanceof Error ? error.message : String(error),
						status: 'failure',
					});
					// Throwing will cause the message to be retried
					throw error;
				}
			}),
		);

		// Log summary
		const successful = results.filter((r) => r.status === 'fulfilled').length;
		const failed = results.filter((r) => r.status === 'rejected').length;

		logger.info('Batch processing complete', {
			batchSize: batch.messages.length,
			successful,
			failed,
			status: failed > 0 ? 'partial' : 'success',
		});
	});
}

/**
 * Process a single index job
 */
async function processIndexJob(job: IndexJob, env: Env, correlationId?: string): Promise<void> {
	switch (job.type) {
		case 'build_index':
			await processBuildIndexJob(job, env, correlationId);
			break;
		case 'maintain_index':
			await processIndexMaintenanceJob(job, env, correlationId);
			break;
		case 'reshard_table':
			await processReshardTableJob(job as ReshardTableJob, env, correlationId);
			break;
		case 'resharding_change_log':
			// Change log entries are batched and processed during Phase 3C
			// They don't need individual processing - they're fetched from queue during replay
			logger.debug('Change log entry queued for later replay', {
				reshardingId: (job as ReshardingChangeLog).resharding_id,
			});
			break;
		default:
			throw new Error(`Unknown job type: ${(job as any).type}`);
	}
}

/**
 * Build a virtual index from existing data
 *
 * Process:
 * 1. Get all shards for this table from topology
 * 2. For each distinct value in the indexed column:
 *    - Query all shards to find which contain that value
 *    - Create index entry mapping value → shard_ids
 * 3. Update index status to 'ready' or 'failed'
 */
async function processBuildIndexJob(job: IndexBuildJob, env: Env, correlationId?: string): Promise<void> {
	const columnList = job.columns.join(', ');
	logger.setTags({
		table: job.table_name,
		indexName: job.index_name,
	});

	logger.info('Building virtual index', {
		indexName: job.index_name,
		table: job.table_name,
		columns: columnList,
	});

	const topologyId = env.TOPOLOGY.idFromName(job.database_id);
	const topologyStub = env.TOPOLOGY.get(topologyId);

	try {
		// 1. Get all shards for this table from topology
		const topology = await topologyStub.getTopology();
		const tableShards = topology.table_shards.filter((s) => s.table_name === job.table_name);

		if (tableShards.length === 0) {
			logger.error('No shards found for table', { table: job.table_name });
			throw new Error(`No shards found for table '${job.table_name}'`);
		}

		logger.info('Found shards for table', {
			shardCount: tableShards.length,
		});

		// 2. Collect all distinct values from all shards
		// Map: composite key value → Set<shard_id>
		const valueToShards = new Map<string, Set<number>>();

		for (const shard of tableShards) {
			try {
				// Get storage stub for this shard
				const storageId = env.STORAGE.idFromName(shard.node_id);
				const storageStub = env.STORAGE.get(storageId);

				// Query for all distinct combinations of indexed columns
				const result = await storageStub.executeQuery({
					query: `SELECT DISTINCT ${columnList} FROM ${job.table_name}`,
					params: [],
					queryType: 'SELECT',
				});

				// Type guard: single query always returns QueryResult
				if (!('rows' in result)) {
					throw new Error('Expected QueryResult but got BatchQueryResult');
				}

				// Extract rows with proper typing to avoid deep instantiation with Disposable types
				const resultRows = (result as any).rows as Record<string, any>[];

				// For each distinct combination, add this shard to its shard set
				for (const row of resultRows) {
					// Build composite key from all indexed columns
					// For single column: just the value
					// For multiple columns: JSON array of values
					let keyValue: string;

					if (job.columns.length === 1) {
						const value: any = row[job.columns[0]!];
						// Skip NULL values
						if (value === null || value === undefined) {
							continue;
						}
						keyValue = String(value);
					} else {
						// Composite index - build key from all column values
						const values = job.columns.map(col => row[col]);
						// Skip if any value is NULL
						if (values.some(v => v === null || v === undefined)) {
							continue;
						}
						// Store as JSON array
						keyValue = JSON.stringify(values);
					}

					if (!valueToShards.has(keyValue)) {
						valueToShards.set(keyValue, new Set());
					}

					valueToShards.get(keyValue)!.add(shard.shard_id);
				}

				console.log(`Processed shard ${shard.shard_id}, found values for index`);
			} catch (error) {
				console.error(`Error querying shard ${shard.shard_id}:`, error);
				throw new Error(`Failed to query shard ${shard.shard_id}: ${error instanceof Error ? error.message : String(error)}`);
			}
		}

		logger.info('Collected distinct values from shards', {
			distinctValues: valueToShards.size,
		});

		// 3. Create index entries in batch
		const entries = Array.from(valueToShards.entries()).map(([keyValue, shardIdSet]) => ({
			keyValue,
			shardIds: Array.from(shardIdSet).sort((a, b) => a - b), // Sort for consistency
		}));

		if (entries.length > 0) {
			const result = await topologyStub.batchUpsertIndexEntries(job.index_name, entries);
			logger.info('Created index entries', {
				entryCount: result.count,
			});
		}

		// 4. Update index status to 'ready'
		await topologyStub.updateIndexStatus(job.index_name, 'ready');

		logger.info('Index build complete', {
			indexName: job.index_name,
			uniqueValues: entries.length,
			status: 'ready',
		});
	} catch (error) {
		logger.error('Failed to build index', {
			indexName: job.index_name,
			error: error instanceof Error ? error.message : String(error),
		});

		// Update index status to 'failed'
		const errorMessage = error instanceof Error ? error.message : String(error);
		await topologyStub.updateIndexStatus(job.index_name, 'failed', errorMessage);

		throw error;
	}
}

/**
 * Maintain virtual indexes after UPDATE/DELETE operations
 *
 * Process:
 * 1. For each affected shard, read current data
 * 2. Compute what index entries should exist based on current state
 * 3. Compare with topology's current state
 * 4. Apply changes in a single batched call
 */
async function processIndexMaintenanceJob(job: IndexMaintenanceJob, env: Env, correlationId?: string): Promise<void> {
	logger.setTags({
		table: job.table_name,
	});

	logger.info('Maintaining indexes after write operation', {
		operation: job.operation,
		affectedIndexes: job.affected_indexes.length,
		shardCount: job.shard_ids.length,
	});
	console.log(`Maintaining indexes for ${job.operation} on ${job.table_name}, shards: ${job.shard_ids.join(',')}`);

	const topologyStub = env.TOPOLOGY.get(env.TOPOLOGY.idFromName(job.database_id));
	const topology = await topologyStub.getTopology();

	// Get index definitions
	const indexes = topology.virtual_indexes.filter(
		(idx) => job.affected_indexes.includes(idx.index_name) && idx.status === 'ready'
	);

	if (indexes.length === 0) {
		console.log('No ready indexes to maintain');
		return;
	}

	// Collect all index changes across all shards
	const allChanges: Array<{
		operation: 'add' | 'remove';
		index_name: string;
		key_value: string;
		shard_id: number;
	}> = [];

	// Process each affected shard
	for (const shardId of job.shard_ids) {
		const shard = topology.table_shards.find((s) => s.table_name === job.table_name && s.shard_id === shardId);
		if (!shard) {
			console.warn(`Shard ${shardId} not found for table ${job.table_name}`);
			continue;
		}

		const storageStub = env.STORAGE.get(env.STORAGE.idFromName(shard.node_id));

		// For each index, rebuild entries for this shard
		for (const index of indexes) {
			const indexColumns = JSON.parse(index.columns);

			// Read current state from shard
			const columnList = indexColumns.join(', ');
			const query = `SELECT ${columnList} FROM ${job.table_name}`;

			try {
				const result = await storageStub.executeQuery({
					query,
					params: [],
					queryType: 'SELECT',
				});

				// Type guard: single query always returns QueryResult
				if (!('rows' in result)) {
					throw new Error('Expected QueryResult but got BatchQueryResult');
				}

				// Extract rows with proper typing to avoid deep instantiation with Disposable types
				const resultRows = (result as any).rows as Record<string, any>[];

				// Build set of values that SHOULD include this shard
				const shouldExist = new Set<string>();
				for (const row of resultRows) {
					const keyValue = computeIndexKey(row, indexColumns);
					if (keyValue) {
						shouldExist.add(keyValue);
					}
				}

				// Get current state from Topology
				const currentEntries = await topologyStub.getIndexEntriesForShard(index.index_name, shardId);
				const currentlyExists = new Set(currentEntries.map((e) => e.key_value));

				// Compute changes
				// Add shard to entries where it should exist but doesn't
				for (const keyValue of shouldExist) {
					if (!currentlyExists.has(keyValue)) {
						allChanges.push({
							operation: 'add',
							index_name: index.index_name,
							key_value: keyValue,
							shard_id: shardId,
						});
					}
				}

				// Remove shard from entries where it shouldn't exist but does
				for (const keyValue of currentlyExists) {
					if (!shouldExist.has(keyValue)) {
						allChanges.push({
							operation: 'remove',
							index_name: index.index_name,
							key_value: keyValue,
							shard_id: shardId,
						});
					}
				}
			} catch (error) {
				console.error(`Error querying shard ${shardId}:`, error);
				throw error;
			}
		}
	}

	// Apply all changes in a single Topology call
	if (allChanges.length > 0) {
		await topologyStub.batchMaintainIndexes(allChanges);
		console.log(`Applied ${allChanges.length} index changes for ${job.table_name}`);
	} else {
		console.log(`No index changes needed for ${job.table_name}`);
	}
}

/**
 * Process resharding job (Phases 3B-5: Copy, Replay, Verify, Switch, Cleanup)
 *
 * Phase 3A (write logging) is handled by the Conductor before queuing
 * Phase 3B: Copy data from source shard to all target shards
 * Phase 3C: Replay all captured changes to target shards
 * Phase 3D: Verify row counts match between source and targets
 * Phase 4: Atomic status switch (mark targets active, source to_be_deleted)
 * Phase 5: Delete old shard data
 */
async function processReshardTableJob(job: ReshardTableJob, env: Env, correlationId?: string): Promise<void> {
	return withLogTags({ source: 'QueueConsumer' }, async () => {
		const tableName = job.table_name;
		const sourceShardId = job.source_shard_id;
		const targetShardIds = job.target_shard_ids;
		const jobId = job.change_log_id; // Use change_log_id as unique job identifier

		logger.setTags({
			table: tableName,
			reshardingId: job.change_log_id,
			jobId,
			correlationId,
		});

		logger.info('Processing resharding job', {
			table: tableName,
			sourceShardId,
			targetShardIds,
			changeLogId: job.change_log_id,
		});

		const topologyId = env.TOPOLOGY.idFromName(job.database_id);
		const topologyStub = env.TOPOLOGY.get(topologyId);

	try {
		// Create async job record
		await topologyStub.createAsyncJob(jobId, 'reshard_table', tableName, {
			source_shard_id: sourceShardId,
			target_shard_ids: targetShardIds,
			shard_key: job.shard_key,
			shard_strategy: job.shard_strategy,
		});

		// Mark async job as running
		await topologyStub.startAsyncJob(jobId);

		// Mark resharding as starting (transition to 'copying' phase)
		await topologyStub.startResharding(tableName);
		logger.info('Resharding started', { table: tableName, status: 'copying' });

		// Phase 3B: Copy data from source shard to target shards
		const copyStats = await copyShardData(
			env,
			job.database_id,
			sourceShardId,
			targetShardIds,
			tableName,
			job.shard_key,
			job.shard_strategy
		);
		logger.info('Phase 3B: Data copy completed', { ...copyStats });

		// Phase 3C: Replay captured changes
		const replayStats = await replayChangeLog(
			env,
			job.database_id,
			job.change_log_id,
			targetShardIds,
			tableName,
			job.shard_key,
			job.shard_strategy
		);
		logger.info('Phase 3C: Change log replay completed', { ...replayStats });

		// Phase 3D: Verify data integrity
		const verifyStats = await verifyIntegrity(
			env,
			job.database_id,
			sourceShardId,
			targetShardIds,
			tableName
		);
		logger.info('Phase 3D: Data verification completed', { ...verifyStats });

		if (!verifyStats.isValid) {
			throw new Error(`Data verification failed: ${verifyStats.error}`);
		}

		// Phase 4: Atomic status switch
		await topologyStub.atomicStatusSwitch(tableName);
		logger.info('Phase 4: Atomic status switch completed', { table: tableName });

		// Phase 5: Delete old shard data
		await deleteOldShardData(env, job.database_id, sourceShardId, tableName);
		logger.info('Phase 5: Old shard data deleted', { table: tableName, shardId: sourceShardId });

		// Mark resharding as complete
		await topologyStub.markReshardingComplete(tableName);
		logger.info('Resharding completed successfully', {
			table: tableName,
			status: 'complete',
		});

		// Mark async job as completed
		await topologyStub.completeAsyncJob(jobId);
	} catch (error) {
		const errorMessage = error instanceof Error ? error.message : String(error);
		logger.error('Resharding failed', {
			table: tableName,
			error: errorMessage,
			status: 'failed',
		});

		// Mark resharding state as failed
		await topologyStub.markReshardingFailed(tableName, errorMessage);

		// Mark all target shards as 'failed' to clean up pending shards
		try {
			await topologyStub.markTargetShardsAsFailed(tableName, targetShardIds);
			logger.info('Target shards marked as failed', { table: tableName, targetShardIds });
		} catch (shardError) {
			logger.warn('Failed to mark target shards as failed', {
				tableName,
				targetShardIds,
				error: shardError instanceof Error ? shardError.message : String(shardError),
			});
		}

		// Increment retry counter
		try {
			await topologyStub.incrementAsyncJobRetries(jobId);
			logger.info('Async job retry counter incremented', { jobId });
		} catch (retryError) {
			logger.warn('Failed to increment async job retries', {
				jobId,
				error: retryError instanceof Error ? retryError.message : String(retryError),
			});
		}

		// Mark async job as failed
		try {
			await topologyStub.failAsyncJob(jobId, errorMessage);
		} catch (jobError) {
			logger.warn('Failed to mark async job as failed', {
				jobId,
				error: jobError instanceof Error ? jobError.message : String(jobError),
			});
		}

		throw error;
	}
	});
}

/**
 * Phase 3B: Copy data from source shard to target shards
 * Routes rows to target shards based on shard key using the same hashing strategy
 */
async function copyShardData(
	env: Env,
	databaseId: string,
	sourceShardId: number,
	targetShardIds: number[],
	tableName: string,
	shardKey: string,
	shardStrategy: 'hash' | 'range'
): Promise<{ rows_copied: number; duration: number }> {
	const startTime = Date.now();
	let rowsCopied = 0;

	logger.info('Phase 3B: Starting data copy', {
		sourceShardId,
		targetShardIds,
	});

	try {
		// Get all data from source shard
		const topologyId = env.TOPOLOGY.idFromName(databaseId);
		const topologyStub = env.TOPOLOGY.get(topologyId);
		const topology = await topologyStub.getTopology();

		const sourceShard = topology.table_shards.find(
			s => s.table_name === tableName && s.shard_id === sourceShardId
		);
		if (!sourceShard) {
			throw new Error(`Source shard ${sourceShardId} not found`);
		}

		const storageId = env.STORAGE.idFromName(sourceShard.node_id);
		const storageStub = env.STORAGE.get(storageId);

		// Query all data from source shard filtered by _virtualShard
		const result = await storageStub.executeQuery({
			query: `SELECT * FROM ${tableName} WHERE _virtualShard = ?`,
			params: [sourceShardId],
			queryType: 'SELECT',
		});

		if (!('rows' in result)) {
			throw new Error('Expected QueryResult but got BatchQueryResult');
		}

		const rows = (result as any).rows as Record<string, any>[];

		// Also get count from source to verify (filtered by _virtualShard)
		const sourceCountResult = await storageStub.executeQuery({
			query: `SELECT COUNT(*) as count FROM ${tableName} WHERE _virtualShard = ?`,
			params: [sourceShardId],
			queryType: 'SELECT',
		});
		const sourceCount = (sourceCountResult as any).rows[0]?.count || 0;

		logger.info('Fetched rows from source shard', {
			rowCount: rows.length,
			countQuery: sourceCount,
			match: rows.length === sourceCount,
		});

		// Warn if there's a mismatch
		if (rows.length !== sourceCount) {
			logger.warn('Row count mismatch in source shard fetch', {
				selectStar: rows.length,
				countQuery: sourceCount,
				difference: sourceCount - rows.length,
			});
		}

		// Build map of target shard IDs to storage nodes for efficient lookup
		const targetShardMap = new Map<number, string>();
		for (const shard of topology.table_shards.filter(s => s.table_name === tableName && targetShardIds.includes(s.shard_id))) {
			targetShardMap.set(shard.shard_id, shard.node_id);
		}

		// Verify all target shards have nodes
		for (const targetShardId of targetShardIds) {
			if (!targetShardMap.has(targetShardId)) {
				throw new Error(`Target shard ${targetShardId} has no node mapping`);
			}
		}

		// Insert rows into target shards based on shard key distribution
		// Each row goes to exactly one target shard determined by hash(shardKeyValue) % targetShardCount
		const shardDistribution = new Map<number, number>();
		for (const targetShardId of targetShardIds) {
			shardDistribution.set(targetShardId, 0);
		}

		for (const row of rows) {
			// Extract the shard key value from the row
			const shardKeyValue = row[shardKey];
			if (shardKeyValue === undefined) {
				logger.warn('Row missing shard key', { shardKey, row });
				continue;
			}

			// Determine which target shard this row belongs to
			// hashToShard returns 0-indexed value, so we use it to select from targetShardIds
			const targetShardIndex = hashToShard(shardKeyValue, targetShardIds.length);
			const targetShardId = targetShardIds[targetShardIndex];

			// Get the storage node for this target shard
			const targetNodeId = targetShardMap.get(targetShardId);
			if (!targetNodeId) {
				throw new Error(`Target shard ${targetShardId} node mapping not found`);
			}

			// Build INSERT OR REPLACE query from row (handles duplicates gracefully)
			// This is idempotent - replaying the copy is safe if it fails partway through
			// IMPORTANT: Replace _virtualShard value with target shard ID for proper isolation
			const columns = Object.keys(row).filter(col => col !== '_virtualShard');
			const allColumns = [...columns, '_virtualShard'];
			const values = allColumns.map(() => '?');
			const query = `INSERT OR REPLACE INTO ${tableName} (${allColumns.join(', ')}) VALUES (${values.join(', ')})`;
			const params = [...columns.map(col => row[col]), targetShardId];

			// Insert row into its designated target shard only
			const targetStorageId = env.STORAGE.idFromName(targetNodeId);
			const targetStorageStub = env.STORAGE.get(targetStorageId);

			try {
				await targetStorageStub.executeQuery({
					query,
					params,
					queryType: 'INSERT',
				});

				rowsCopied++;
				const currentCount = shardDistribution.get(targetShardId) || 0;
				shardDistribution.set(targetShardId, currentCount + 1);
			} catch (error) {
				logger.error('Failed to copy row to target shard', {
					shardKeyValue,
					targetShardId,
					error: error instanceof Error ? error.message : String(error),
				});
				throw error;
			}
		}

		// Log final distribution
		let distributionLog = 'Final shard distribution: ';
		for (const [shardId, count] of shardDistribution) {
			distributionLog += `Shard ${shardId}: ${count} rows | `;
		}
		logger.info(distributionLog, { totalRows: rowsCopied, totalShards: targetShardIds.length });

		const duration = Date.now() - startTime;
		logger.info('Phase 3B: Data copy completed', { rowsCopied, duration });
		return { rows_copied: rowsCopied, duration };
	} catch (error) {
		logger.error('Phase 3B: Data copy failed', {
			error: error instanceof Error ? error.message : String(error),
		});
		throw error;
	}
}

/**
 * Phase 3C: Replay captured changes to target shards
 * This is idempotent - replaying INSERT again will fail (which is okay for copy),
 * but UPDATE and DELETE work fine
 */
async function replayChangeLog(
	env: Env,
	databaseId: string,
	changeLogId: string,
	targetShardIds: number[],
	tableName: string,
	shardKey: string,
	shardStrategy: 'hash' | 'range'
): Promise<{ changes_replayed: number; duration: number }> {
	const startTime = Date.now();
	let changesReplayed = 0;

	logger.info('Phase 3C: Starting change log replay', {
		changeLogId,
		targetShardIds,
	});

	try {
		// In production, this would fetch change log entries from a persistent log
		// For now, we'll just log that replay would happen here
		logger.info('Phase 3C: No change log entries to replay (or would fetch from persistent log)');

		const duration = Date.now() - startTime;
		return { changes_replayed: changesReplayed, duration };
	} catch (error) {
		logger.error('Phase 3C: Change log replay failed', {
			error: error instanceof Error ? error.message : String(error),
		});
		throw error;
	}
}

/**
 * Phase 3D: Verify data integrity by comparing row counts
 */
async function verifyIntegrity(
	env: Env,
	databaseId: string,
	sourceShardId: number,
	targetShardIds: number[],
	tableName: string
): Promise<{ isValid: boolean; error?: string; duration: number }> {
	const startTime = Date.now();

	logger.info('Phase 3D: Starting data verification', {
		sourceShardId,
		targetShardIds,
	});

	try {
		const topologyId = env.TOPOLOGY.idFromName(databaseId);
		const topologyStub = env.TOPOLOGY.get(topologyId);
		const topology = await topologyStub.getTopology();

		// Get source shard row count
		const sourceShard = topology.table_shards.find(
			s => s.table_name === tableName && s.shard_id === sourceShardId
		);
		if (!sourceShard) {
			throw new Error(`Source shard ${sourceShardId} not found`);
		}

		const sourceStorageId = env.STORAGE.idFromName(sourceShard.node_id);
		const sourceStorageStub = env.STORAGE.get(sourceStorageId);

		// Count rows in source shard filtered by _virtualShard to ensure we only count source data
		const sourceCountResult = await sourceStorageStub.executeQuery({
			query: `SELECT COUNT(*) as count FROM ${tableName} WHERE _virtualShard = ?`,
			params: [sourceShardId],
			queryType: 'SELECT',
		});

		const sourceCount = (sourceCountResult as any).rows[0]?.count || 0;
		logger.info('Source shard row count', { sourceShardId, count: sourceCount, nodeId: sourceShard.node_id });

		// Get target shards total row count with detailed breakdown
		let targetTotalCount = 0;
		const targetBreakdown: { [key: number]: number } = {};

		for (const targetShardId of targetShardIds) {
			const targetShard = topology.table_shards.find(
				s => s.table_name === tableName && s.shard_id === targetShardId
			);
			if (!targetShard) {
				throw new Error(`Target shard ${targetShardId} not found`);
			}

			const targetStorageId = env.STORAGE.idFromName(targetShard.node_id);
			const targetStorageStub = env.STORAGE.get(targetStorageId);

			// Count rows in target shard filtered by _virtualShard to ensure we only count target data
			const targetCountResult = await targetStorageStub.executeQuery({
				query: `SELECT COUNT(*) as count FROM ${tableName} WHERE _virtualShard = ?`,
				params: [targetShardId],
				queryType: 'SELECT',
			});

			const targetCount = (targetCountResult as any).rows[0]?.count || 0;
			targetTotalCount += targetCount;
			targetBreakdown[targetShardId] = targetCount;

			logger.info('Target shard row count', {
				targetShardId,
				count: targetCount,
				nodeId: targetShard.node_id,
				percentageOfTarget: ((targetCount / Math.max(sourceCount, 1)) * 100).toFixed(2) + '%'
			});
		}

		const duration = Date.now() - startTime;

		// Log detailed breakdown
		logger.info('Phase 3D: Verification breakdown', {
			sourceCount,
			targetTotalCount,
			difference: targetTotalCount - sourceCount,
			targetBreakdown,
			sourceShardId,
			targetShardIds,
			duration,
		});

		if (sourceCount !== targetTotalCount) {
			const error = `Row count mismatch: source=${sourceCount}, targets=${targetTotalCount}`;
			logger.warn('Phase 3D: Data verification failed', {
				error,
				duration,
				breakdown: targetBreakdown,
				difference: targetTotalCount - sourceCount,
			});
			return { isValid: false, error, duration };
		}

		logger.info('Phase 3D: Data verification passed', { rowCount: sourceCount, duration });
		return { isValid: true, duration };
	} catch (error) {
		logger.error('Phase 3D: Data verification error', {
			error: error instanceof Error ? error.message : String(error),
		});
		throw error;
	}
}

/**
 * Phase 5: Delete old shard data
 */
async function deleteOldShardData(
	env: Env,
	databaseId: string,
	sourceShardId: number,
	tableName: string
): Promise<void> {
	logger.info('Phase 5: Starting old shard deletion', {
		sourceShardId,
	});

	try {
		const topologyId = env.TOPOLOGY.idFromName(databaseId);
		const topologyStub = env.TOPOLOGY.get(topologyId);
		const topology = await topologyStub.getTopology();

		const sourceShard = topology.table_shards.find(
			s => s.table_name === tableName && s.shard_id === sourceShardId
		);
		if (!sourceShard) {
			throw new Error(`Source shard ${sourceShardId} not found`);
		}

		const storageId = env.STORAGE.idFromName(sourceShard.node_id);
		const storageStub = env.STORAGE.get(storageId);

		// Delete all data from source shard
		await storageStub.executeQuery({
			query: `DELETE FROM ${tableName}`,
			params: [],
			queryType: 'DELETE',
		});

		logger.info('Phase 5: Old shard data deleted', { sourceShardId });
	} catch (error) {
		logger.error('Phase 5: Old shard deletion failed', {
			error: error instanceof Error ? error.message : String(error),
		});
		throw error;
	}
}

/**
 * Compute index key from a row and column list
 * Returns null if any column value is NULL (NULL values are not indexed)
 */
function computeIndexKey(row: Record<string, any>, columns: string[]): string | null {
	const values = [];
	for (const col of columns) {
		const val = row[col];
		if (val === null || val === undefined) {
			return null; // Don't index NULL values
		}
		values.push(val);
	}
	return columns.length === 1 ? String(values[0]) : JSON.stringify(values);
}

